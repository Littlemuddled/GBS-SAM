import os
import numpy as np
from sklearn.metrics import accuracy_score
import cv2


def compute_segmentation_metrics(true_mask, pred_mask, num_classes=3):
    """
    è®¡ç®—å¤šç±»åˆ«è¯­ä¹‰åˆ†å‰²è¯„ä¼°æŒ‡æ ‡ï¼šAccuracyã€Precisionã€Recallã€F1ã€Diceã€IoUï¼ˆå¿½ç•¥èƒŒæ™¯ç±»0ï¼‰

    å‚æ•°ï¼š
        true_mask: H x W numpy arrayï¼ŒçœŸå®æ ‡ç­¾
        pred_mask: H x W numpy arrayï¼Œé¢„æµ‹æ ‡ç­¾
        num_classes: ç±»åˆ«æ•°ï¼ˆåƒç´ å€¼èŒƒå›´ 0~num_classes-1ï¼‰

    è¿”å›ï¼š
        å­—å…¸å½¢å¼çš„å„é¡¹æŒ‡æ ‡ï¼ˆæ¯ç±»+å‡å€¼ï¼Œä¸åŒ…æ‹¬èƒŒæ™¯ç±»ï¼‰
    """
    metrics = {
        'accuracy': 0.0,
        'precision': [],
        'recall': [],
        'f1': [],
        'dice': [],
        'iou': []
    }

    # å±•å¹³
    true_flat = true_mask.flatten()
    pred_flat = pred_mask.flatten()

    # æ€»ä½“å‡†ç¡®ç‡ï¼ˆåªè€ƒè™‘éèƒŒæ™¯ç±»ï¼‰
    non_bg_mask = true_flat != 0
    if np.sum(non_bg_mask) > 0:
        metrics['accuracy'] = accuracy_score(true_flat[non_bg_mask], pred_flat[non_bg_mask])
    else:
        metrics['accuracy'] = 0.0

    # ä»ç±»1å¼€å§‹ï¼Œå¿½ç•¥èƒŒæ™¯ç±»0
    for cls in range(1, num_classes):
        true_cls = (true_flat == cls).astype(np.uint8)
        pred_cls = (pred_flat == cls).astype(np.uint8)

        TP = np.sum((true_cls == 1) & (pred_cls == 1))
        FP = np.sum((true_cls == 0) & (pred_cls == 1))
        FN = np.sum((true_cls == 1) & (pred_cls == 0))

        precision = TP / (TP + FP + 1e-8)
        recall = TP / (TP + FN + 1e-8)
        f1 = 2 * precision * recall / (precision + recall + 1e-8)
        dice = 2 * TP / (2 * TP + FP + FN + 1e-8)
        iou = TP / (TP + FP + FN + 1e-8)

        metrics['precision'].append(precision)
        metrics['recall'].append(recall)
        metrics['f1'].append(f1)
        metrics['dice'].append(dice)
        metrics['iou'].append(iou)

    # è®¡ç®—å‡å€¼
    metrics['mean_precision'] = np.mean(metrics['precision'])
    metrics['mean_recall'] = np.mean(metrics['recall'])
    metrics['mean_f1'] = np.mean(metrics['f1'])
    metrics['mean_dice'] = np.mean(metrics['dice'])
    metrics['mean_iou'] = np.mean(metrics['iou'])

    return metrics


def evaluate_all(true_dir, pred_dir, num_classes=3):
    results = []

    file_list = sorted(os.listdir(true_dir))
    for file in file_list:
        true_path = os.path.join(true_dir, file)
        pred_path = os.path.join(pred_dir, file)
        if not os.path.exists(pred_path):
            print(f"âŒ é¢„æµ‹æ–‡ä»¶ä¸å­˜åœ¨: {file}")
            continue

        true_mask = cv2.imread(true_path, cv2.IMREAD_GRAYSCALE)
        pred_mask = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)

        if true_mask.shape != pred_mask.shape:
            print(f"âš ï¸ å°ºå¯¸ä¸åŒ¹é…: {file}")
            continue

        metrics = compute_segmentation_metrics(true_mask, pred_mask, num_classes)
        result = {
            'file': file,
            'accuracy': metrics['accuracy'],
            'mean_precision': metrics['mean_precision'],
            'mean_recall': metrics['mean_recall'],
            'mean_f1': metrics['mean_f1'],
            'mean_dice': metrics['mean_dice'],
            'mean_iou': metrics['mean_iou'],
        }

        # è®°å½•éèƒŒæ™¯ç±»çš„æŒ‡æ ‡
        for i, cls in enumerate(range(1, num_classes)):
            result[f'precision_class_{cls}'] = metrics['precision'][i]
            result[f'recall_class_{cls}'] = metrics['recall'][i]
            result[f'f1_class_{cls}'] = metrics['f1'][i]
            result[f'dice_class_{cls}'] = metrics['dice'][i]
            result[f'iou_class_{cls}'] = metrics['iou'][i]

        results.append(result)

    # æ‰“å°å¹³å‡æŒ‡æ ‡
    print("\nğŸ“Š å¹³å‡æŒ‡æ ‡ï¼ˆAcross all images, ignoring background classï¼‰ï¼š")
    avg_metrics = {
        'accuracy': np.mean([r['accuracy'] for r in results]),
        'mean_precision': np.mean([r['mean_precision'] for r in results]),
        'mean_recall': np.mean([r['mean_recall'] for r in results]),
        'mean_f1': np.mean([r['mean_f1'] for r in results]),
        'mean_dice': np.mean([r['mean_dice'] for r in results]),
        'mean_iou': np.mean([r['mean_iou'] for r in results]),
    }
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.4f}")


if __name__ == '__main__':
    true_dir = r"data_covid/val/masks"
    pred_dir = r"data_covid/val/mask_pred"
    evaluate_all(true_dir, pred_dir, num_classes=3)